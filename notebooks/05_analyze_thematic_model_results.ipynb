{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from itertools import combinations\n",
    "import logging\n",
    "\n",
    "BASE_REPORT_DIR = Path(\"../data/reports/supervised_learning\")\n",
    "THEMATIC_MODELS_DIR = BASE_REPORT_DIR / \"thematic_models\"\n",
    "MASTER_RESULTS_PATH = BASE_REPORT_DIR / \"master_results_summary.csv\"\n",
    "SP500_PRICE_DATA_PATH = Path(\"../data/processed/regime_identification/smoothed/summaries/sp500_ret126d_logvol21d_3states_smoothed200_full_data_with_states.csv\")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "def load_all_oof_predictions(base_dir: Path) -> dict:\n",
    "    oof_data = {}\n",
    "    oof_files = list(base_dir.glob(\"**/cv_results/oof_predictions.csv\"))\n",
    "    \n",
    "    if not oof_files:\n",
    "        logging.warning(f\"No oof_predictions.csv files found in {base_dir}\")\n",
    "        return {}\n",
    "        \n",
    "    logging.info(f\"Found {len(oof_files)} OOF prediction files.\")\n",
    "    \n",
    "    for file_path in oof_files:\n",
    "        try:\n",
    "            run_name = file_path.parent.parent.name\n",
    "            \n",
    "            df = pd.read_csv(file_path, parse_dates=['date'])\n",
    "            df.set_index('date', inplace=True)\n",
    "            oof_data[run_name] = df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to load or process {file_path}: {e}\")\n",
    "            \n",
    "    return oof_data\n",
    "\n",
    "all_oof_data = load_all_oof_predictions(THEMATIC_MODELS_DIR)\n",
    "\n",
    "if MASTER_RESULTS_PATH.exists():\n",
    "    master_results_df = pd.read_csv(MASTER_RESULTS_PATH)\n",
    "    logging.info(f\"Loaded master results summary with shape: {master_results_df.shape}\")\n",
    "else:\n",
    "    master_results_df = pd.DataFrame()\n",
    "    logging.warning(f\"Master results summary not found at {MASTER_RESULTS_PATH}\")\n",
    "\n",
    "if SP500_PRICE_DATA_PATH.exists():\n",
    "    sp500_df = pd.read_csv(SP500_PRICE_DATA_PATH, parse_dates=['date'], index_col='date')[['sp500_adjusted_close']]\n",
    "    logging.info(f\"Loaded S&P 500 price data with shape: {sp500_df.shape}\")\n",
    "else:\n",
    "    sp500_df = pd.DataFrame()\n",
    "    logging.warning(f\"S&P 500 price data not found at {SP500_PRICE_DATA_PATH}\")\n",
    "\n",
    "if all_oof_data:\n",
    "    sample_key = list(all_oof_data.keys())[0]\n",
    "    print(f\"Sample OOF data from '{sample_key}':\")\n",
    "    display(all_oof_data[sample_key].head())\n",
    "else:\n",
    "    print(\"No OOF data was loaded.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "85123898f30ae113",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def calculate_model_agreement(oof_data: dict) -> pd.DataFrame:\n",
    "    model_names = list(oof_data.keys())\n",
    "    agreement_matrix = pd.DataFrame(np.eye(len(model_names)), index=model_names, columns=model_names)\n",
    "\n",
    "    all_predictions = pd.DataFrame({name: df['predicted_label'] for name, df in oof_data.items()})\n",
    "\n",
    "    for model1, model2 in combinations(model_names, 2):\n",
    "        aligned_preds = all_predictions[[model1, model2]].dropna()\n",
    "        \n",
    "        if not aligned_preds.empty:\n",
    "            agreement = np.mean(aligned_preds[model1] == aligned_preds[model2])\n",
    "            agreement_matrix.loc[model1, model2] = agreement\n",
    "            agreement_matrix.loc[model2, model1] = agreement\n",
    "        else:\n",
    "            agreement_matrix.loc[model1, model2] = np.nan\n",
    "            agreement_matrix.loc[model2, model1] = np.nan\n",
    "            \n",
    "    return agreement_matrix\n",
    "\n",
    "if all_oof_data:\n",
    "    agreement_df = calculate_model_agreement(all_oof_data)\n",
    "\n",
    "    plt.figure(figsize=(18, 15))\n",
    "    sns.heatmap(\n",
    "        agreement_df, \n",
    "        annot=True, \n",
    "        cmap=\"viridis\", \n",
    "        fmt=\".2f\",\n",
    "        linewidths=.5\n",
    "    )\n",
    "    plt.title(\"Pairwise Model Prediction Agreement (%)\", fontsize=16)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Cannot calculate model agreement without OOF data.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42a60410504ffdd2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def analyze_prediction_stability(oof_data: dict) -> pd.DataFrame:\n",
    "    stability_results = []\n",
    "\n",
    "    for name, df in oof_data.items():\n",
    "        preds = df['predicted_label'].dropna()\n",
    "        if preds.empty:\n",
    "            continue\n",
    "            \n",
    "        flips = (preds.diff() != 0)\n",
    "        flip_rate = flips.mean() \n",
    "\n",
    "        blocks = flips.cumsum()\n",
    "        spell_durations = preds.groupby(blocks).size()\n",
    "        \n",
    "        if not spell_durations.empty:\n",
    "            avg_spell_duration = spell_durations.mean()\n",
    "            std_spell_duration = spell_durations.std()\n",
    "            min_spell_duration = spell_durations.min()\n",
    "            max_spell_duration = spell_durations.max()\n",
    "        else:\n",
    "            avg_spell_duration = std_spell_duration = min_spell_duration = max_spell_duration = np.nan\n",
    "\n",
    "        stability_results.append({\n",
    "            'model_run': name,\n",
    "            'flip_rate': flip_rate,\n",
    "            'avg_spell_duration_days': avg_spell_duration,\n",
    "            'std_spell_duration': std_spell_duration,\n",
    "            'min_spell_duration_days': min_spell_duration,\n",
    "            'max_spell_duration_days': max_spell_duration,\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(stability_results)\n",
    "\n",
    "if all_oof_data:\n",
    "    stability_df = analyze_prediction_stability(all_oof_data)\n",
    "    stability_df['model_type'] = stability_df['model_run'].apply(lambda x: x.split('_')[-1])\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    sns.barplot(data=stability_df.sort_values('flip_rate'), x='flip_rate', y='model_run', hue='model_type', dodge=False)\n",
    "    plt.title('Prediction Flip Rate (Lower is More Stable)', fontsize=16)\n",
    "    plt.xlabel('Fraction of Days Prediction Changed')\n",
    "    plt.ylabel('Model Run')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(14, 7))\n",
    "    sns.barplot(data=stability_df.sort_values('avg_spell_duration_days', ascending=False), x='avg_spell_duration_days', y='model_run', hue='model_type', dodge=False)\n",
    "    plt.title('Average Prediction Spell Duration (Higher is More Stable)', fontsize=16)\n",
    "    plt.xlabel('Average Consecutive Days with Same Prediction')\n",
    "    plt.ylabel('Model Run')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n--- Stability Metrics Summary ---\")\n",
    "    display(stability_df.sort_values('flip_rate').set_index('model_run'))\n",
    "else:\n",
    "    print(\"Cannot analyze stability without OOF data.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bebd3813f7fa47c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def analyze_prediction_confidence(oof_data: dict) -> pd.DataFrame:\n",
    "    confidence_results = []\n",
    "\n",
    "    for name, df in oof_data.items():\n",
    "        df_clean = df.dropna(subset=['predicted_label', 'true_label']).copy()\n",
    "        proba_cols = [c for c in df_clean.columns if c.startswith('proba_class_')]\n",
    "        if not proba_cols or df_clean.empty:\n",
    "            continue\n",
    "            \n",
    "        predicted_labels_int = df_clean['predicted_label'].astype(int)\n",
    "        df_clean['confidence'] = df_clean[proba_cols].values[np.arange(len(df_clean)), predicted_labels_int]\n",
    "        \n",
    "        df_clean['is_correct'] = (df_clean['predicted_label'] == df_clean['true_label'])\n",
    "        \n",
    "        avg_confidence = df_clean['confidence'].mean()\n",
    "        std_confidence = df_clean['confidence'].std()\n",
    "        \n",
    "        avg_conf_correct = df_clean[df_clean['is_correct']]['confidence'].mean()\n",
    "        avg_conf_incorrect = df_clean[~df_clean['is_correct']]['confidence'].mean()\n",
    "        \n",
    "        confidence_results.append({\n",
    "            'model_run': name,\n",
    "            'avg_confidence': avg_confidence,\n",
    "            'std_confidence': std_confidence,\n",
    "            'avg_conf_when_correct': avg_conf_correct,\n",
    "            'avg_conf_when_incorrect': avg_conf_incorrect,\n",
    "            'confidence_lift': avg_conf_correct - avg_conf_incorrect,\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(confidence_results)\n",
    "\n",
    "if all_oof_data:\n",
    "    confidence_df = analyze_prediction_confidence(all_oof_data)\n",
    "    \n",
    "    confidence_melted = confidence_df.melt(\n",
    "        id_vars='model_run', \n",
    "        value_vars=['avg_conf_when_correct', 'avg_conf_when_incorrect'],\n",
    "        var_name='condition', \n",
    "        value_name='average_confidence'\n",
    "    )\n",
    "    confidence_melted['condition'] = confidence_melted['condition'].map({\n",
    "        'avg_conf_when_correct': 'Correct',\n",
    "        'avg_conf_when_incorrect': 'Incorrect'\n",
    "    })\n",
    "\n",
    "    plt.figure(figsize=(10, 12))\n",
    "    sns.barplot(data=confidence_melted, y='model_run', x='average_confidence', hue='condition', palette={'Correct': 'g', 'Incorrect': 'r'})\n",
    "    plt.title('Model Confidence: Correct vs. Incorrect Predictions', fontsize=16)\n",
    "    plt.xlabel('Average Prediction Probability')\n",
    "    plt.ylabel('Model Run')\n",
    "    plt.xlim(0, 1)\n",
    "    plt.legend(title='Prediction Outcome')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n--- Confidence Metrics Summary (Higher Lift is Better) ---\")\n",
    "    display(confidence_df.sort_values('confidence_lift', ascending=False).set_index('model_run'))\n",
    "else:\n",
    "    print(\"Cannot analyze confidence without OOF data.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb2348eb88e27c6b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if not master_results_df.empty:\n",
    "    master_results_df['model_type'] = master_results_df['model'].apply(lambda x: 'lightgbm' if 'lightgbm' in x.lower() else x)\n",
    "\n",
    "    agg_by_model = master_results_df.groupby('model_type').agg(\n",
    "        avg_f1_macro=('f1_macro_mean', 'mean'),\n",
    "        std_of_f1_macro_means=('f1_macro_mean', 'std'),\n",
    "        avg_mcc=('mcc_mean', 'mean'),\n",
    "        std_of_mcc_means=('mcc_mean', 'std'),\n",
    "        avg_auc=('auc_roc_ovr_macro_mean', 'mean'),\n",
    "        std_of_auc_means=('auc_roc_ovr_macro_mean', 'std'),\n",
    "        num_models=('model', 'count')\n",
    "    ).sort_values('avg_f1_macro', ascending=False)\n",
    "\n",
    "    print(\"\\n--- Average Performance by Algorithm Type ---\")\n",
    "    display(agg_by_model)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 6), sharey=True)\n",
    "    fig.suptitle('Algorithm Performance Comparison Across All Themes', fontsize=16)\n",
    "    \n",
    "    metrics_to_plot = [('avg_f1_macro', 'F1 Macro'), ('avg_mcc', 'Matthews Corr Coef'), ('avg_auc', 'AUC ROC (OvR Macro)')]\n",
    "    \n",
    "    for i, (metric, title) in enumerate(metrics_to_plot):\n",
    "        sns.barplot(data=agg_by_model.reset_index(), x='model_type', y=metric, ax=axes[i], palette='viridis')\n",
    "        axes[i].set_title(title)\n",
    "        axes[i].set_xlabel('Algorithm')\n",
    "        axes[i].set_ylabel('Average Score')\n",
    "        axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Cannot analyze performance by algorithm without master results summary.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9021b12b887c7ed",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def plot_ensemble_predictions_over_time(oof_data: dict, price_data: pd.DataFrame):\n",
    "    if not oof_data or price_data.empty:\n",
    "        logging.warning(\"Cannot plot ensemble predictions without OOF and price data.\")\n",
    "        return\n",
    "\n",
    "    all_preds_df = pd.DataFrame({name: df['predicted_label'] for name, df in oof_data.items()})\n",
    "    all_preds_df = all_preds_df.ffill().bfill()\n",
    "\n",
    "    majority_vote = all_preds_df.mode(axis=1)[0].astype(int)\n",
    "    agreement_score = all_preds_df.apply(lambda row: row.value_counts(normalize=True).max(), axis=1)\n",
    "\n",
    "    plot_df = price_data.copy()\n",
    "    plot_df = plot_df.join(all_preds_df.iloc[:, 0].to_frame('true_label_sample')) \n",
    "    \n",
    "    true_label_series = list(oof_data.values())[0]['true_label']\n",
    "    \n",
    "    plot_df = plot_df.join(true_label_series)\n",
    "    plot_df = plot_df.join(majority_vote.rename('majority_vote'))\n",
    "    plot_df = plot_df.join(agreement_score.rename('agreement'))\n",
    "    plot_df.dropna(subset=['sp500_adjusted_close', 'true_label', 'majority_vote'], inplace=True)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(20, 12), sharex=True, gridspec_kw={'height_ratios': [3, 1]})\n",
    "    \n",
    "    ax1.plot(plot_df.index, plot_df['sp500_adjusted_close'], color='black', lw=0.5, label='S&P 500')\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.set_title('Ensemble Prediction vs. True Regime Over Time', fontsize=16)\n",
    "    ax1.set_ylabel('S&P 500 (Log Scale)')\n",
    "    \n",
    "    regime_colors = {0: 'red', 1: 'green', 2: 'gray'}\n",
    "    ax1.fill_between(plot_df.index, 0, plot_df['sp500_adjusted_close'].max()*1.1, where=plot_df['true_label'] == 0, \n",
    "                     facecolor=regime_colors[0], alpha=0.15, label='True Regime 0 (Bear)')\n",
    "    ax1.fill_between(plot_df.index, 0, plot_df['sp500_adjusted_close'].max()*1.1, where=plot_df['true_label'] == 1, \n",
    "                     facecolor=regime_colors[1], alpha=0.15, label='True Regime 1 (Bull)')\n",
    "    ax1.fill_between(plot_df.index, 0, plot_df['sp500_adjusted_close'].max()*1.1, where=plot_df['true_label'] == 2, \n",
    "                     facecolor=regime_colors[2], alpha=0.15, label='True Regime 2 (Neutral)')\n",
    "\n",
    "    ax1.scatter(plot_df.index, plot_df['sp500_adjusted_close'], c=plot_df['majority_vote'].map(regime_colors), \n",
    "                marker='.', s=10, label='Majority Vote Prediction')\n",
    "    ax1.legend()\n",
    "    ax1.grid(which='both', linestyle='--', alpha=0.5)\n",
    "\n",
    "    ax2.plot(plot_df.index, plot_df['agreement'], label='Model Agreement', color='purple', lw=1.5)\n",
    "    ax2.set_title('Model Agreement Score', fontsize=14)\n",
    "    ax2.set_ylabel('Agreement (0.0 to 1.0)')\n",
    "    ax2.set_ylim(0, 1.05)\n",
    "    ax2.axhline(y=1/3, color='r', linestyle='--', lw=1, label='Random Chance')\n",
    "    ax2.grid(True)\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if all_oof_data and not sp500_df.empty:\n",
    "    plot_ensemble_predictions_over_time(all_oof_data, sp500_df)\n",
    "else:\n",
    "    print(\"Cannot generate ensemble time-series plot without OOF and Price data.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad00009fc2a391ca",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
